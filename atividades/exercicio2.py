"""
Exerc√≠cio 2 - √Årvore de Decis√£o

1. CONJUNTO DE DADOS ESCOLHIDO:
   - Dataset: Iris (sklearn.datasets.load_iris)
   - 150 amostras de flores Iris de 3 esp√©cies diferentes
   - 4 caracter√≠sticas: comprimento e largura das s√©palas e p√©talas
   - 3 classes: Setosa, Versicolor, Virginica

2. PROFUNDIDADE DA √ÅRVORE (PODA):
   - F√≥rmula: altura_dela = 1 + √∫ltimo_d√≠gito_de_seu_RA % 4
   - Assumindo √∫ltimo d√≠gito do RA = 8 (ajuste conforme seu RA)
   - C√°lculo: 1 + (8 % 4) = 0
   - max_depth = 1

3. PROCESSO DECIS√ìRIO (exemplo de caminho):
   Caminho 1: Raiz ‚Üí Folha Esquerda
   - A √°rvore avalia: "petal width (cm) <= 0.8?"
   - Se SIM: classifica como Setosa
   - Explica√ß√£o: Flores com p√©talas muito estreitas (‚â§ 0.8 cm) s√£o sempre Setosa

   Caminho 2: Raiz ‚Üí Folha Direita
   - A √°rvore avalia: "petal width (cm) <= 0.8?"
   - Se N√ÉO: classifica como Versicolor/Virginica (necessita mais profundidade para separar)
   - Explica√ß√£o: Flores com p√©talas mais largas (> 0.8 cm) podem ser Versicolor ou Virginica

4. SIGNIFICADO DOS VALORES NAS FOLHAS:
   Cada folha mostra:
   - gini: √çndice de impureza de Gini (0 = puro, >0 = misturado)
     * Mede a probabilidade de classifica√ß√£o incorreta
     * gini = 1 - Œ£(pi¬≤), onde pi √© a propor√ß√£o de cada classe

   - samples: N√∫mero de amostras que chegaram nesta folha
     * Total de exemplos de treinamento que seguiram este caminho

   - value: [#setosa, #versicolor, #virginica]
     * Quantidade de amostras de cada classe nesta folha
     * Exemplo: [50, 0, 0] = 50 Setosa, 0 Versicolor, 0 Virginica

   - class: Classe prevista (maioria)
     * A classe com maior n√∫mero de amostras nesta folha
"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# 1. Carregar conjunto de dados Iris
iris = load_iris()
X, y = iris.data, iris.target

# 2. Criar e treinar √°rvore com poda
# IMPORTANTE: Ajuste o √∫ltimo d√≠gito do seu RA aqui
ultimo_digito_RA = 8  # <<< ALTERE ESTE VALOR PARA O √öLTIMO D√çGITO DO SEU RA
max_depth = 1 + (ultimo_digito_RA % 4)

print(f"√öltimo d√≠gito do RA: {ultimo_digito_RA}")
print(f"Profundidade da √°rvore: max_depth = 1 + ({ultimo_digito_RA} % 4) = {max_depth}")
print()

model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
model.fit(X, y)

# 3. Visualizar a √°rvore de decis√£o
plt.figure(figsize=(20, 12))
plot_tree(
    model,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    filled=True,
    rounded=True,
    fontsize=12,
)
plt.title(f"√Årvore de Decis√£o - Dataset Iris (max_depth={max_depth})", fontsize=16)
plt.tight_layout()
plt.savefig("arvore_decisao_iris.png", dpi=300, bbox_inches="tight")
print("√Årvore salva em: arvore_decisao_iris.png")
plt.show()

# 4. Informa√ß√µes adicionais sobre a √°rvore
print("\n=== INFORMA√á√ïES DA √ÅRVORE ===")
print(f"N√∫mero de folhas: {model.get_n_leaves()}")
print(f"Profundidade: {model.get_depth()}")
print(f"Acur√°cia no treino: {model.score(X, y):.4f}")
print(
    f"Feature mais importante: {iris.feature_names[model.feature_importances_.argmax()]}"
)

# 5. EXPLICA√á√ÉO DETALHADA DO PROCESSO DECIS√ìRIO
print("\n" + "=" * 70)
print("EXPLICA√á√ÉO DETALHADA DO PROCESSO DECIS√ìRIO")
print("=" * 70)

# Obter informa√ß√µes sobre a √°rvore
tree = model.tree_
feature = tree.feature
threshold = tree.threshold
n_nodes = tree.node_count

print(f"\nüìä A √°rvore possui {n_nodes} n√≥s no total")
print(f"üåø N√≥ raiz usa a feature: {iris.feature_names[feature[0]]}")
print(f"üî¢ Limiar de decis√£o: {threshold[0]:.2f} cm")

# Exemplo de classifica√ß√£o de amostras
print("\n" + "-" * 70)
print("EXEMPLO PR√ÅTICO - CLASSIFICANDO AMOSTRAS:")
print("-" * 70)

exemplos = [0, 50, 100]  # √çndices de exemplos de cada classe
for idx in exemplos:
    amostra = X[idx]
    classe_real = iris.target_names[y[idx]]
    classe_pred = iris.target_names[model.predict([amostra])[0]]

    print(f"\nüå∏ Amostra {idx} - Classe real: {classe_real}")
    print(f"   Caracter√≠sticas:")
    for i, nome in enumerate(iris.feature_names):
        print(f"   - {nome}: {amostra[i]:.1f} cm")

    # Simular o caminho de decis√£o
    print(f"\n   üîç Processo de Decis√£o:")
    print(
        f"   1. A √°rvore pergunta: '{iris.feature_names[feature[0]]}' <= {threshold[0]:.2f}?"
    )

    valor_decisao = amostra[feature[0]]
    if valor_decisao <= threshold[0]:
        print(f"   2. Resposta: SIM ({valor_decisao:.2f} <= {threshold[0]:.2f})")
        print(f"   3. Segue para FOLHA ESQUERDA")
    else:
        print(f"   2. Resposta: N√ÉO ({valor_decisao:.2f} > {threshold[0]:.2f})")
        print(f"   3. Segue para FOLHA DIREITA")

    print(f"   ‚úÖ Classifica√ß√£o final: {classe_pred}")
    print(f"   {'‚úì CORRETO' if classe_real == classe_pred else '‚úó INCORRETO'}")

# Explica√ß√£o dos valores nas folhas
print("\n" + "=" * 70)
print("SIGNIFICADO DETALHADO DOS VALORES NAS FOLHAS")
print("=" * 70)

# Informa√ß√µes sobre as folhas
n_leaves = model.get_n_leaves()
children_left = tree.children_left
children_right = tree.children_right
values = tree.value

print(f"\nüçÉ A √°rvore possui {n_leaves} folhas")

folha_num = 1
for i in range(n_nodes):
    # Verificar se √© folha (sem filhos)
    if children_left[i] == children_right[i]:  # √â uma folha
        samples_node = tree.n_node_samples[i]
        impurity = tree.impurity[i]
        value_counts = values[i][0]

        print(f"\nüìç FOLHA {folha_num}:")
        print(f"   ‚Ä¢ samples = {samples_node}")
        print(
            f"     ‚îî‚îÄ Significado: {samples_node} amostras de treinamento chegaram aqui"
        )

        print(f"\n   ‚Ä¢ value = {value_counts.astype(int).tolist()}")
        print(f"     ‚îî‚îÄ Distribui√ß√£o das classes:")
        for j, classe in enumerate(iris.target_names):
            count = int(value_counts[j])
            percent = (count / samples_node * 100) if samples_node > 0 else 0
            print(f"        {classe}: {count} amostras ({percent:.1f}%)")

        print(f"\n   ‚Ä¢ gini = {impurity:.4f}")
        print(f"     ‚îî‚îÄ √çndice de impureza de Gini")
        if impurity == 0:
            print(f"        Gini = 0 ‚Üí Folha PURA (100% de uma √∫nica classe)")
        else:
            print(f"        Gini > 0 ‚Üí Folha MISTA (cont√©m m√∫ltiplas classes)")
            # Calcular gini manualmente para demonstrar
            probs = value_counts / samples_node
            gini_calc = 1 - sum(probs**2)
            print(
                f"        C√°lculo: 1 - Œ£(pi¬≤) = 1 - ({' + '.join([f'{p:.3f}¬≤' for p in probs])})"
            )
            print(f"                              = {gini_calc:.4f}")

        classe_majoritaria = iris.target_names[value_counts.argmax()]
        print(f"\n   ‚Ä¢ class = {classe_majoritaria}")
        print(f"     ‚îî‚îÄ Classe prevista (maioria dos votos)")

        folha_num += 1

print("\n" + "=" * 70)
print("üí° RESUMO DAS EXPLICA√á√ïES")
print("=" * 70)
print(
    """
1. PROCESSO DECIS√ìRIO:
   A √°rvore toma decis√µes sequenciais baseadas em perguntas simples
   do tipo "feature X <= valor?". Cada resposta SIM/N√ÉO leva a um
   caminho diferente at√© chegar em uma folha com a classifica√ß√£o final.

2. VALORES NAS FOLHAS (o que significam):
   
   üìä samples: Quantas amostras do conjunto de treino chegaram nesta folha
   
   üìà value: Array com contagem de cada classe [setosa, versicolor, virginica]
             Mostra a distribui√ß√£o das classes que ca√≠ram nesta folha
   
   üéØ gini: Medida de impureza (0 = puro, 1 = m√°xima mistura)
            Quanto mais pr√≥ximo de 0, mais confi√°vel √© a classifica√ß√£o
            
   üè∑Ô∏è  class: Classe final prevista (a que tem maior contagem no value)

3. EXEMPLO DE INTERPRETA√á√ÉO:
   Se uma folha tem: samples=50, value=[50, 0, 0], gini=0.0, class=setosa
   
   Significa: 50 amostras chegaram aqui, todas eram setosa (50/0/0),
             a folha √© pura (gini=0), ent√£o classifica como setosa.
"""
)
